<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>GNN: Graph Neural Networks</title>
    <url>/2023/08/08/GNN-Graph-Neural-Networks/</url>
    <content><![CDATA[<h2 id="GNN-Graph-Neural-Networks"><a href="#GNN-Graph-Neural-Networks" class="headerlink" title="GNN: Graph Neural Networks"></a>GNN: Graph Neural Networks</h2><blockquote>
<p>《An gentle introduction to graph neural networks》</p>
<p>——GNN被用于处理图结构的数据；图结构数据如何表示为tensor；GNN如何处理图数据；GNN网络是由什么模块组成；背后的思想是什么</p>
<p>《Understanding Convolutions on Graphs》</p>
<p>——讲解Images上的卷积是如何泛化到Graphs上的卷积的。</p>
</blockquote>
<h3 id="Table-content："><a href="#Table-content：" class="headerlink" title="Table content："></a>Table content：</h3><ol>
<li><p>what kind of data is phrased as a graph</p>
</li>
<li><p>Difference between graphs and other types of data</p>
</li>
<li><p>Building a morden GNN</p>
</li>
<li><p>A GNN playground with real-world task and dataset</p>
</li>
</ol>
<h3 id="1-Introduction-to-Graphs"><a href="#1-Introduction-to-Graphs" class="headerlink" title="1. Introduction to Graphs"></a>1. Introduction to Graphs</h3><ul>
<li>A graph represents the relations(edges) between a collection of entities(nodes). 一般来说，一个图可以表示为V、E和U，如下所示：</li>
</ul>
<p><img src="/images/blogs/GNN-Graph-Neural-Networks/image-20230719111108017.png" alt="image-20230719111108017"></p>
<p>——其中，attributes表示每个节点、每条边、整个图所表示的<strong>信息</strong>。为了进一步定量地表示这些信息，我们可以用（不同维度的）向量来表示每个节点、每条边、整个图所表示的<strong>信息</strong>。如下所示：</p>
<p><img src="/images/blogs/GNN-Graph-Neural-Networks/image-20230719111437197.png" alt="image-20230719111437197"></p>
<p>——其中，左上角的节点的信息用一个长度为6的向量表示，高矮表示特征值的大小。重点在于，这些向量能否很好地定量表示图的信息，<strong>GNN要如何学习到这些向量</strong>。</p>
<ul>
<li>根据edge是否有方向，可以将graph分为directed graph和undirected graph，如下所示：</li>
</ul>
<p><img src="/images/blogs/GNN-Graph-Neural-Networks/image-20230719112047988.png" alt="image-20230719112047988"></p>
<h3 id="2-Represent-Data-as-Graphs"><a href="#2-Represent-Data-as-Graphs" class="headerlink" title="2. Represent Data as Graphs"></a>2. Represent Data as Graphs</h3><h4 id="Images-as-Graphs"><a href="#Images-as-Graphs" class="headerlink" title="Images as Graphs"></a>Images as Graphs</h4><ul>
<li>Typically，我们将images表示为带通道的矩阵，即三维tensor(eg. 224*224*3)</li>
<li>将images表示为整齐的graph：<ul>
<li>每个pixel看作一个节点</li>
<li>每个pixel的RGB值形成一个三维向量，作为该节点的向量表示</li>
<li>每个pixel与邻接pixel之间形成边(<strong>undirected</strong>)。这样每个non-border pixel都有8个邻居节点。</li>
<li>如何表示边？❤</li>
</ul>
</li>
</ul>
<p><img src="/images/blogs/GNN-Graph-Neural-Networks/image-20230719114043583.png" alt="image-20230719114043583"></p>
<ul>
<li>image的三种表示，依次为：图片像素值、邻接矩阵（大小为nodes*nodes）、graph</li>
</ul>
<h4 id="Texts-as-Graphs"><a href="#Texts-as-Graphs" class="headerlink" title="Texts as Graphs"></a>Texts as Graphs</h4><ul>
<li><p>Typically，我们将一段文本划分为token，将每个token映射为索引，然后将这段文本表示为索引序列。</p>
</li>
<li><p>将text表示为graph：</p>
<ul>
<li>每个token作为一个节点，词向量可以作为节点向量</li>
<li>text sequence是一个单向的序列，因此其中的边是有向边；每个非结尾的token应该有一条有向边指向与其相邻的下一个token</li>
<li>如何表示边？❤</li>
</ul>
</li>
</ul>
<h4 id="Graph-valued-Data-in-the-Wild"><a href="#Graph-valued-Data-in-the-Wild" class="headerlink" title="Graph-valued Data in the Wild"></a>Graph-valued Data in the Wild</h4><ul>
<li>text和image通常不会表示成graph，因为text和image本身已经是非常规则的数据了，表示成graph时，其邻接矩阵会非常稀疏，比如image的邻接矩阵是带状，而text的邻接矩阵是对角线</li>
<li>而有些数据，很难用除了graph以外的数据形式来表示</li>
<li>分子图：每个原子作为一个节点，原子间的化学键作为边</li>
<li>社交网络图：每个人作为一个节点，如果两个人之间有交互行为，则在这两个点之间构建一条边</li>
<li>引用图：每篇论文作为一个节点，论文A引用论文B则有一条A到B的有向边</li>
</ul>
<h3 id="3-Different-Tasks-on-Graphs：Problems"><a href="#3-Different-Tasks-on-Graphs：Problems" class="headerlink" title="3. Different Tasks on Graphs：Problems"></a>3. Different Tasks on Graphs：Problems</h3><h4 id="Graph-level-task"><a href="#Graph-level-task" class="headerlink" title="Graph-level task"></a>Graph-level task</h4><ul>
<li>在图级别任务中，我们的目标是<strong>预测整个图的属性</strong>。例如，对于以图形表示的分子，我们可能想要预测分子的气味，或者它是否会结合到与疾病相关的受体上。(classification)</li>
</ul>
<p><img src="/images/blogs/GNN-Graph-Neural-Networks/image-20230720113247688.png" alt="image-20230720113247688"></p>
<h4 id="Node-level-task"><a href="#Node-level-task" class="headerlink" title="Node-level task"></a>Node-level task</h4><ul>
<li>节点级任务关注于<strong>预测图中每个节点的身份或角色</strong>。</li>
</ul>
<p><img src="/images/blogs/GNN-Graph-Neural-Networks/image-20230720113727209.png" alt="image-20230720113727209"></p>
<ul>
<li>按照图像类比，节点级预测问题类似于图像分割，我们试图标记图像中每个像素的作用。对于文本，类似的任务是预测句子中每个单词的词性。(细粒度的classfication)</li>
</ul>
<h4 id="Edge-level-task"><a href="#Edge-level-task" class="headerlink" title="Edge-level task"></a>Edge-level task</h4><ul>
<li>边级别任务关注于<strong>预测节点之间是否有边存在，以及边的属性如何</strong>，即预测nodes之间是否存在关系，存在哪种关系。</li>
<li>一个具体的例子是图片场景理解，要判断节点之间的关系，可以先给定包含所有结点的全连接图，然后根据模型对边的预测值来修剪边，得到稀疏的图：</li>
</ul>
<p><img src="/images/blogs/GNN-Graph-Neural-Networks/image-20230720114134124.png" alt="image-20230720114134124"></p>
<p><img src="/images/blogs/GNN-Graph-Neural-Networks/image-20230720114214671.png" alt="image-20230720114214671"></p>
<p>如何用GNN解决different tasks on graph?</p>
<ol>
<li>如何把graph表示为tensor，与neural network兼容</li>
<li>如何设计GNN来处理表示graph的tensor</li>
<li>GNN的不同模块对prediction起到什么作用</li>
</ol>
<h3 id="4-Represent-Graphs-as-Tensors"><a href="#4-Represent-Graphs-as-Tensors" class="headerlink" title="4. Represent Graphs as Tensors"></a>4. Represent Graphs as Tensors</h3><blockquote>
<p>So, how do we go about solving these different graph tasks with neural networks? The first step is to think about how we will represent graphs to be compatible with neural networks.</p>
</blockquote>
<ul>
<li>Graphs have up to four types of information that we will potentially want to use to make predictions: <strong>nodes, edges, global-context and connectivity.</strong></li>
<li>The first three are relatively straightforward: for example, with nodes we can form a node feature matrix $N$ by assigning each node an index $i$ and storing the feature for $node_i$ in $N$.</li>
<li>But, these matrices have a variable number of examples.</li>
<li><p><strong>Representing a graph’s connectivity</strong> is more complicated.</p>
<ul>
<li><p>use an adjacency matrix：</p>
<ul>
<li>easily tensorisable</li>
<li>leads to very sparse adjacency matrices, which are space-inefficient</li>
<li>not permutation invariant（many adjacency matrices that can encode the same connectivity）</li>
</ul>
</li>
<li><p>use an adjacency lists：</p>
<ul>
<li>avoid computation and storage on the disconnected parts of the graph</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="/images/blogs/GNN-Graph-Neural-Networks/image-20230725220911217.png" alt="image-20230725220911217"></p>
<ul>
<li>It should be noted that the figure uses scalar values per node/edge/global, but most practical tensor representations have vectors per graph attribute.</li>
</ul>
<h3 id="5-Graph-Neural-Network"><a href="#5-Graph-Neural-Network" class="headerlink" title="5. Graph Neural Network"></a>5. Graph Neural Network</h3><ul>
<li><strong>A GNN is an optimizable transformation on all attributes of the graph (nodes, edges, global-context) that preserves graph symmetries.</strong></li>
<li>GNNs adopt <strong>a “graph-in, graph-out” architecture</strong> meaning that these model types accept a graph as input, with information loaded into its nodes, edges and global-context, and progressively transform these embeddings, without changing the connectivity of the input graph.</li>
</ul>
<h4 id="The-simplest-GNN"><a href="#The-simplest-GNN" class="headerlink" title="The simplest GNN"></a>The simplest GNN</h4><ul>
<li>The simplest GNN architecture, one where we learn new embeddings for all graph attributes (nodes, edges, global), but where we do not yet use the connectivity of the graph.</li>
<li>This GNN uses <strong>a separate multilayer perceptron (MLP) on each component of a graph</strong>; we call this a GNN layer. Layer $N$ of this GNN includes $f<em>{U_n}, f</em>{V<em>n}, f</em>{E_n}$.</li>
</ul>
<p><img src="/images/blogs/GNN-Graph-Neural-Networks/image-20230726094537247.png" alt="image-20230726094537247"></p>
<ul>
<li>As is common with neural networks modules or layers, we can <strong>stack these GNN layers</strong> together. (需要pooling层，否则MLP堆叠没有意义)</li>
<li>Because a GNN does <strong>not update the connectivity of the input graph</strong>, we can describe the output graph of a GNN with the same adjacency list and the same number of feature vectors as the input graph.</li>
</ul>
<h4 id="GNN-Predictions-by-Pooling-Information"><a href="#GNN-Predictions-by-Pooling-Information" class="headerlink" title="GNN Predictions by Pooling Information"></a>GNN Predictions by Pooling Information</h4><ul>
<li>使用上述的GNN model来解决tasks，比如一个nodel-level的binary classification</li>
<li>直观的，因为graph中包含了每个node的embedding，我们可以用final layer输出的graph信息，将每个node embedding投入一个linear classifier，获得分类结果，如下图：</li>
</ul>
<p><img src="/images/blogs/GNN-Graph-Neural-Networks/image-20230726095501604.png" alt="image-20230726095501604"></p>
<ul>
<li>但如果我们没有node embeddings（或出于某种原因不能使用），只能用相关的其他信息，比如edges embeddings来解决node-level binary classification任务，then we need a way to <strong>collect information from edges and give them to nodes for prediction</strong>. We can do this by <strong><em>pooling</em></strong>.</li>
<li>Pooling proceeds in two steps: <strong><em>gather</em></strong> and <strong><em>aggregated</em></strong>.</li>
<li>We represent the <em>pooling</em> operation by the letter $\rho$, and denote that we are gathering information from edges to nodes as $\rho<em>{E_n\rightarrow V</em>{n}}$.</li>
</ul>
<p><img src="/images/blogs/GNN-Graph-Neural-Networks/image-20230726100842458.png" alt="image-20230726100842458"></p>
<ul>
<li>这里节点明明只有两条邻接边，为什么gather的embedding有3个？❤</li>
<li>Pool edge embeddings into nodes to make nodel-level predictions, denoted as $\rho<em>{E_n\rightarrow V</em>{n}}$:</li>
</ul>
<p><img src="/images/blogs/GNN-Graph-Neural-Networks/image-20230726101108039.png" alt="image-20230726101108039"></p>
<ul>
<li>Pool node embeddings into edges to make edge-level predictions, denoted as $\rho<em>{V_n\rightarrow E</em>{n}}$:</li>
</ul>
<p><img src="/images/blogs/GNN-Graph-Neural-Networks/image-20230726101323227.png" alt="image-20230726101323227"></p>
<ul>
<li>Pool node/edge embeddings into global context to make global predictions, denoted as $\rho<em>{V_n\rightarrow U</em>{n}}$：节点矩阵到global embeddings的具体汇聚操作是什么？如果二者的维度不一致呢❤</li>
</ul>
<p><img src="/images/blogs/GNN-Graph-Neural-Networks/image-20230726104148023.png" alt="image-20230726104148023"></p>
<ul>
<li>End-to-end Simplest GNN：</li>
</ul>
<p><img src="/images/blogs/GNN-Graph-Neural-Networks/image-20230726104508078.png" alt="image-20230726104508078"></p>
<ul>
<li><p>Note that in this simplest GNN formulation, we’re not using the connectivity of the graph at all inside the GNN layer. We only use connectivity when pooling information for prediction. This pooling technique will serve as a building block for constructing more sophisticated GNN models.</p>
</li>
<li><p>如何在GNN中构建使用connectivity信息的模块？使用Pool或者message pass。两者的区别在于，pool是不同的attributes，相邻的entity之间进行信息传递；message pass是相同的attribute，相邻的entity之间进行信息传递。how to apply pool or message pass within GNN layer ❤</p>
</li>
</ul>
<h4 id="Passing-messages-between-parts-of-the-graph"><a href="#Passing-messages-between-parts-of-the-graph" class="headerlink" title="Passing messages between parts of the graph"></a>Passing messages between parts of the graph</h4><ul>
<li>In order to make our learned embeddings aware of graph connectivity，we can do this by using <em>message passing</em>, where neighboring nodes or edges exchange information and influence each other’s updated embeddings.</li>
<li>Message passing works in three steps: <strong><em>gather</em></strong>($g$), <strong><em>aggregate</em></strong> and <strong><em>update</em></strong>. This sequence of operations, when applied once, is the simplest type of <strong>message-passing GNN layer</strong>.</li>
<li>Node-message-passing GNN layer：</li>
</ul>
<p><img src="/images/blogs/GNN-Graph-Neural-Networks/image-20230726110637076.png" alt="image-20230726110637076"></p>
<ul>
<li><p>This is reminiscent of standard convolution: in essence, <strong><em>message passing and convolution are operations to aggregate and process the information of an element’s neighbors in order to update the element’s value</em></strong>. In graphs, the element is a node, and in images, the element is a pixel. However, the number of neighboring nodes in a graph can be variable, unlike in an image where each pixel has a set number of neighboring elements.</p>
</li>
<li><p>By <strong><em>stacking message passing GNN layers</em></strong> together, <strong><em>a node can eventually incorporate information from across the entire graph</em></strong>: after three layers, a node has information about the nodes three steps away from it. 就像感受野一样，最终可以获得全局图片的信息。</p>
</li>
</ul>
<p><img src="/images/blogs/GNN-Graph-Neural-Networks/image-20230726111530116.png" alt="image-20230726111530116"></p>
<ul>
<li>graph convolutional layer是什么？是主要指embedding transformation，message-passing或pooling操作可以嵌入其中吗？❤</li>
</ul>
<h3 id="GNN-playground"><a href="#GNN-playground" class="headerlink" title="GNN playground"></a>GNN playground</h3><h4 id="1-Task"><a href="#1-Task" class="headerlink" title="1. Task"></a>1. Task</h4><ul>
<li>a graph-level prediction task</li>
<li>we consider only a single binary label per molecule, classifying if a molecular graph smells “pungent” or not, as labeled by a professional perfumer. </li>
</ul>
<h4 id="2-Dataset"><a href="#2-Dataset" class="headerlink" title="2. Dataset"></a>2. Dataset</h4><ul>
<li><p>Leffingwell Odor Dataset, which is composed of molecules with associated odor percepts (labels)</p>
</li>
<li><p>We represent each molecule as a graph, where <strong>atoms are nodes</strong> containing <strong>a one-hot encoding</strong> for its atomic identity (Carbon, Nitrogen, Oxygen, Fluorine) and <strong>bonds are edges</strong> containing <strong>a one-hot encoding</strong> its bond type (single, double, triple or aromatic)</p>
</li>
</ul>
<h4 id="3-Model"><a href="#3-Model" class="headerlink" title="3. Model"></a>3. Model</h4><ul>
<li>model template: use sequential GNN layers, followed by a linear model with a sigmoid activation for classification</li>
<li>levers that can customize the model:<ul>
<li>The number of GNN layers, also called the <em>depth</em>.</li>
<li>The dimensionality of each attribute when updated. The update function is a 1-layer MLP with a relu activation function and a layer norm for normalization of activations.</li>
<li>The aggregation function used in pooling: max, mean or sum.</li>
<li>The graph attributes that get updated, or styles of message passing: nodes, edges and global representation. We control these via boolean toggles (on or off). A baseline model would be a graph-independent GNN (all message-passing off) which aggregates all data at the end into a single global attribute. Toggling on all message-passing functions yields a GraphNets architecture.</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>沐神开组会</category>
      </categories>
      <tags>
        <tag>GNN</tag>
        <tag>李沐</tag>
      </tags>
  </entry>
  <entry>
    <title>Sunset&#39;s Citywalk</title>
    <url>/2023/08/07/Sunset-s-Citywalk/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2023/08/07/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
</search>
